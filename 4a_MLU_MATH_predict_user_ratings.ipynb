{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLU Logo](https://drive.corp.amazon.com/view/bwernes@/MLU_Logo.png?download=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"0\">MATH - Final Project</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although recommender systems are the secret source for many multi-billion businesses, prototyping a minimum viable recommender system takes only some basic mathematical and statistical fundamentals for machine learning to implement. For the final project, we will start from scratch and walk through the process of how to prototype a book recommender system.\n",
    "\n",
    "* <a href=\"#99\">Business Problem: Recommender System</a> (predict user preferences)\n",
    "    * ML Problem and Data Loading\n",
    "    \n",
    "    \n",
    "* <a href=\"#1\">Question 1</a>: Basic Data Visualization\n",
    "* <a href=\"#2\">Question 2</a>: Data Vectorization\n",
    "* <a href=\"#3\">Question 3</a>: Model Definition\n",
    "* <a href=\"#4\">Question 4</a>: Baseline Model\n",
    "* <a href=\"#5\">Question 5</a>: Model Likelihood\n",
    "* <a href=\"#6\">Question 6</a>: Loss Function\n",
    "* <a href=\"#7\">Question 7</a>: Gradient Descent\n",
    "* <a href=\"#8\">Question 8</a>: Overfitting\n",
    "* <a href=\"#9\">Question 9</a>: Recommendations\n",
    "* <a href=\"#10\">Question 10</a>: Submit to Leaderboard\n",
    "\n",
    "\n",
    "* <a href=\"#100\">Final Comments</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "# Upgrade dependencies\n",
    "#!pip install --upgrade pip\n",
    "#!pip install --upgrade scikit-learn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math as math\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"99\">Business Problem: Recommender System</a> \n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "For your final project, you will leverage your growing mathematical knowledge to build a __recommender system__ similar to those that you find used throughout any digital content provider which will attempt to predict user's preferences based upon past ratings.  The technique we use is called a __model-based  collaborative filtering__ where we use only a user’s past behaviors (items previously purchased and ratings given to those items) to build a model to predict the missing data (ratings for items that the user may have an interest in).\n",
    "\n",
    "\n",
    "#### Data Loading\n",
    "\n",
    "We are using a subset of Amazon reviews for our task.  It is actually based after a [public dataset](http://jmcauley.ucsd.edu/data/amazon/) collected by scraping all reviews prior to 2014, extracting userID, ASIN, and star rating. This dataset itself would be very large and hard to train on owing to the fact there are many ASINs without only one or two reviews, and many users that only review one or two books.  The subset we use is actually a very restricted subset known as the $40$-core.  This is the collection of those ASINs and users such that every ASIN has at least 40 reviews and every user has reviewed at least 40 books (if you are wondering how this was done, see [this notebook](https://eider.corp.amazon.com/bwernes/notebook/output/NB90E71Y0N)). The ratings are on a scale from 1 to 5.\n",
    "\n",
    "Let's load our datasets, ```training``` and ```test_features```. We build the recommender system using the ```training``` data, and evaluate how well the model performs on the ```test_features```, by submitting model predictions to this [leaderboard](https://leaderboard.corp.amazon.com/tasks/665).\n",
    "\n",
    "The code below loads the datasets, stores a list of the unique userIDs and ASINs, counts the number of such userIDs and ASINs, and then splits the ```training``` dataset into two sets: ```train``` (train set) and ```val``` (validation set). We will use the validation set to evaluate model performance before submitting to the leaderboard.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------\n",
      "Sample Training Data\n",
      "-----------\n",
      "      ID            User        ASIN  Rating\n",
      "0  66799  A3NDST1X8JVAP7  1490471103       5\n",
      "1  80385  A1E2T8OSEMVN3H  149473592X       5\n",
      "2  80636  A3BR1CG9CRXB6Z  1494755610       5\n",
      "3  45627   AGYHA346U3CNP  1477694919       2\n",
      "4  78045  A2N0YZV2JT6O72  1493752650       5\n",
      "-----------\n",
      "Sample Test Data\n",
      "-----------\n",
      "      ID            User        ASIN\n",
      "0  84634  A1C7DFNSXTY09X  149617707X\n",
      "1  55852  A13NJZ1M018A7C  1482594129\n",
      "2  28875  A3RINEQ4GDTR55  144235948X\n",
      "3  94134  A1W5758V90WQKZ  B008O0QUU2\n",
      "4  31501  A243FA8XGHI63J  1455574880\n",
      "-----------\n",
      "Number of Users: 1490\n",
      "Number of ASINs: 1186\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "# Import the datasets\n",
    "training = pd.read_csv('../../notebooks/final_project/MATH_Final_Project_Data_training.csv', dtype = {'User': str, 'ASIN': str, 'Rating': np.int})\n",
    "test_features = pd.read_csv('../../notebooks/final_project/MATH_Final_Project_Data_test_features.csv', dtype = {'User': str, 'ASIN': str})\n",
    "\n",
    "print(\"-----------\")\n",
    "print(\"Sample Training Data\")\n",
    "print(\"-----------\")\n",
    "print(training.head(5))\n",
    "\n",
    "print(\"-----------\")\n",
    "print(\"Sample Test Data\")\n",
    "print(\"-----------\")\n",
    "print(test_features.head(5))\n",
    "\n",
    "# Count number of unique users and number of unique ASINs in our dataset\n",
    "uniqueUsers = training['User'].unique().tolist()\n",
    "uniqueASINs = training['ASIN'].unique().tolist()\n",
    "\n",
    "# If using pandas.pivot(), beware that it sorts uniqueUsers and uniqueASINs by default\n",
    "uniqueUsers.sort() # Sort, to avoid index matching errors later on\n",
    "uniqueASINs.sort() # Sort, to avoid index matching errors later on\n",
    "\n",
    "numUser = len(uniqueUsers)\n",
    "numASIN = len(uniqueASINs)\n",
    "\n",
    "print(\"-----------\")\n",
    "print(\"Number of Users: {}\".format(numUser))\n",
    "print(\"Number of ASINs: {}\".format(numASIN))\n",
    "print(\"-----------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Training Data (81472, 4)\n",
      "Shape of Test Features Data (27158, 3)\n",
      "Shape of Train Data (61104, 4)\n",
      "Shape of Validation Data (20368, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split to train and validation\n",
    "train, val = train_test_split(training, random_state = 42, stratify = training['ASIN'])\n",
    "num_train = train.shape[0]\n",
    "num_val = val.shape[0]\n",
    "\n",
    "print(\"Shape of Training Data\", training.shape)\n",
    "print(\"Shape of Test Features Data\", test_features.shape)\n",
    "print(\"Shape of Train Data\", train.shape)\n",
    "print(\"Shape of Validation Data\", val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <a name=\"1\">1 Basic Data Visualization</a> \n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Let's dig in and start to do some data visualization.  When working with real-world data, it is always important to try to understand what bizarre little issues it has.  We've removed many of these for you, but all the same we should do at least a tiny exercise in data visualization so we can see some features of our data.\n",
    "\n",
    "In a real world setting, data collected from feedbacks like user ratings can be very sparse and data points are mostly collected from very popular items (books) and highly engaged users. Large amount of less known items (books) don’t have ratings at all. Let’s see some plots on the distribution of books ratings frequency, to have a __better idea of the data we have__, and ponder whether is enough (and good enough) for a recommender model.\n",
    "\n",
    "\n",
    "###  Project Question 1\n",
    "\n",
    "> Use [MatPlotLib](https://matplotlib.org/) to make the following two plots: the [histogram](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.hist.html) of the number of reviews a user gives, and a [scatter plot](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.scatter.html) of the average rating of an ASIN versus the number of ratings that ASIN received.  It is good pratice to not touch the testing data during model development, so make these plots only on the train data.\n",
    "\n",
    "It is helpful to know you can use things like ```.groupby([fearure1][feature2].count())``` or ```.groupby([feature1][feature2].mean())``` to help with grabbing the data for the desired plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Implement Code for Project Question 1 here\n",
    "train.groupby(['User']).count()['Rating'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = train.groupby(['ASIN']).mean()['Rating']\n",
    "y = train.groupby(['ASIN']).count()['ID']\n",
    "\n",
    "plt.scatter(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <a name=\"2\">2 Data Vectorization</a> \n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Now that we agree we have enough (and good enough) data for a recommender system, we start thinking and planning to build a machine learning model to solve the problem. But, wait, if you go up and look again at the samples from our datasets we printed when we imported them, you'll notice the data it is not a vector or matrix exactly.  Various columns are strings rather than numbers, and even if we solved that (say by creating a unique ID number for every ASIN and user) it would still be in a very strange format.  __Deciding how to represent your data is a major first step in any ML problem, and in this case it is a bit subtle.__  \n",
    "\n",
    "So, first, we need to transform the dataframe of ratings into a proper format that can be consumed by a machine learning model, with a recommender system in mind! We might, in fact, __want the data to be in an $n\\times m$ matrix format__, where $n$ is the number of users and $m$ is the number of ASINs. \n",
    "\n",
    "We can thus think of our data as a score matrix $S = \\left(s_{i,j}\\right)$ which is a $n\\times m$ matrix where\n",
    "$$\n",
    "s_{i,j} = \\begin{cases}\n",
    "s & \\text{if the rating by user $i$ to ASIN $j$ was $s$ stars,}\\\\\n",
    "0 & \\text{otherwise,}\n",
    "\\end{cases}\n",
    "$$\n",
    "where $n$ is the number of users and $m$ is the number of ASINs.\n",
    "\n",
    "As we might notice, our data is extremely sparse, that's to say with many zero elements. That makes sense, after examining the plots from Question 1. Our data is only partially observed, which is to say each user only rated a small number of ASINs. We will also need another matrix, the matrix of which ASINs were rated. \n",
    "\n",
    "Let $R = \\left(r_{i,j}\\right)$ be the matrix where\n",
    "$$\n",
    "r_{i,j} = \\begin{cases}\n",
    "1 & \\text{if user $i$ rated ASIN $j$,}\\\\\n",
    "0 & \\text{otherwise.}\n",
    "\\end{cases}\n",
    "$$\n",
    "This matrix will allow us to mask off which entries were observed or not.\n",
    "\n",
    "These matrices will form the core of the work we do in all future weeks, so it is a good idea to make them (create them, define them, put them together) now.\n",
    "\n",
    "\n",
    "### Project Question 2\n",
    "> Write code to translate our Pandas DataFrames into two numpy matrices.  You'll want to do it for both the train and validation data, so you will end up with four matrices ```R_train```, ```S_train```, ```R_val```, and ```S_val```. Print all four matrices and examine their elements - should be many values of zero!\n",
    "\n",
    "It is helpful to know that other than defining the matrices with for loops or list comprehensions, you can also reshape the dataframes of ratings, by pivoting the dataframes to the desired format with users as rows and ASINs as columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of s_train:  (1490, 1186)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 5. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] \n",
      "\n",
      "Shape of s_val (1490, 1186)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "s_train = train.pivot(index='User', columns='ASIN', values='Rating').to_numpy()\n",
    "s_train[np.isnan(s_train)] = 0\n",
    "\n",
    "s_val = val.pivot(index='User', columns='ASIN', values='Rating').to_numpy()\n",
    "s_val[np.isnan(s_val)] = 0\n",
    "\n",
    "print(\"Shape of s_train: \", s_train.shape)\n",
    "print(s_train,'\\n')\n",
    "print(\"Shape of s_val\", s_val.shape)\n",
    "print(s_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of r_train:  (1490, 1186)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] \n",
      "\n",
      "Shape of r_val (1490, 1186)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "r_train = s_train.copy()\n",
    "r_train[r_train > 0] = 1\n",
    "\n",
    "r_val = s_val.copy()\n",
    "r_val[r_val > 0] = 1\n",
    "\n",
    "print(\"Shape of r_train: \", r_train.shape)\n",
    "print(r_train,'\\n')\n",
    "print(\"Shape of r_val\", r_val.shape)\n",
    "print(r_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <a name=\"3\">3 Model Definition</a> \n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "To better understand how to approach thinking about and building a recommender system, at least by [collaborative filtering](https://en.wikipedia.org/wiki/Collaborative_filtering#), it helps to print and examine the ```S_train```. You'll notice the zero entries. In order to define the model we will be working with, we need to have some way of making predictions  for the unfilled entries - that's the recommender system at the end of the day!  \n",
    "\n",
    "Our goal with this project is to be able to construct a prediction matrix $P = (p_{i,j})$ which fills in all the missing scores where $r_{i,j} = 0$.  If we assumed nothing about how the matrix $P$ is constructed, we would have no basis to build a prediction.  We will need to assume P was generated from some process that has more structure than just an arbitrary matrix.\n",
    "\n",
    "Let's ponder on two things. One, as we observed previously, the $S$ matrices have many zeros - that's good in a way, gives us the opportunity to build the recommender system in the first place. However, the matrices are huge, 1490\n",
    "by 1186 (and they could be even bigger in other practical scenarios!). So, well, first we have this __high dimensions__ fact to handle - remember all the computational impediments related to storing, working, and manipulating large matrices even on current high-dimensionality friednly ML frameworks. Reducing dimensions can improve the performance of the algorithm in terms of both storage and time. We discussed matrix factorization as a simple way to build larger matrices from smaller ones - keep that thought for now!\n",
    "\n",
    "Second, we have to start thinking about how are we going to find out how some users ratings or preferences of some ASINs might impact other unexplored users-ASINs preferences or ratings. We have to somehow find a way to connect users-ASINs-ratings in a meaningful way (well, __mathematically meaningful__ at least) to extract __underlying links__ that might exist between users-ASINs to explain their corresponding ratings, and leverage these __uncovered links__ to predict other users-ASINs interactions. Speaking of matrix factorization, or rather matrix multiplication, two matrices can be multiplied only if the number of columns in the first column is equal to the number of rows in the second matrix. Therefore the two reduced matrices have a common dimension, somehow hidden or covered dimension if you so wish, that we don't really see it in the final matrix product, but it had to be there in order to be able to compute the product to begin with. We thus turn to matrix factorization again - this is the solution to both our problems: reduce the dimensionality of our modeling approach, and actually building a relevant/appropriate model for a recommender system!\n",
    "\n",
    "#### Matrix Factorization\n",
    "\n",
    "Thus, we will make use of matrix multiplication to say that our prediction matrix should be the __product of two other matrices__. Basically, decomposing the users-ASINs interactions matrix into the product of two lower dimensionality rectangular matrices. These matrices actually represent the users and ASINs individually. The first matrix can be seen as the _users matrix_, where rows represent the users, and columns are telling us about some of users' affinities for ASINs - let's call it the *affinities* matrix $A$. The second matrix can be seen as the _ASINs matrix_, where rows tell us about the features ASINs have to offer to users, and columns represent the ASINs - let's call it the *features* matrix $F$. \n",
    "\n",
    "The columns in the users *affinities* matrix $A$ and the rows in the ASINs *features* matrix $F$ are called *latent factors* and are an indication of hidden characteristics about the users or the ASINs, patterns in the data that could lead to more personalized the recommendations. The number of such latent factors can be anything from one to hundreds. This number is one of the things that need to be optimized during the training of the model. \n",
    "\n",
    "The idea we will use is the following:  to each ASIN $j$ we will associate $k$ features $(f_{\\ell,j})_{\\ell = 1}^k$, and to each user $i$ we will associate $k$ affinities $(a_{i,\\ell})_{\\ell=1}^k$.  These features can be thought of as simple descriptors of the book, like for instance amount of action or romance.  The affinities, can be thought of as providing how much a person likes or dislikes a feature so perhaps $+1$ for action since they like action, and $-0.5$ for romance.\n",
    "\n",
    "__The rating that a user will assign to a ASIN will be taken as a sum over all the features of their affinities for that feature times that ASIN's features.__\n",
    "\n",
    "As a matter of useful terminology, in the world of ML, a *parameter* is a number that needs to be fixed in order to specify a model completely.  For instance, in this case every affinity and every feature must be specified for our model to be able to make predictions.  So if every user has two features, and every ASIN has two features, the total number of parameters is:\n",
    "$$\n",
    "\\#\\text{Parameters} = 2\\cdot\\#\\text{User} + 2 \\cdot\\#\\text{ASIN}.\n",
    "$$\n",
    "\n",
    "As a general rule of thumb, the more parameters you have, the more data you need to fit your model (an old statistics rule stated you need about 10 data points for each parameter).  There are exceptions to this (particularly in deep learning), but it is always a good idea to keep the number of parameters always in mind.\n",
    "\n",
    "\n",
    "### Project Question 3\n",
    "> Express the above verbally described model mathematically in matrix notation as a product of the matrices of features and affinities.  What are the dimensions of the matrices involved?  How many parameters are there in this model?  How does the number of parameters compare to the number of entries of $P$?\n",
    "\n",
    "> Complete this question in abstract matrices shapes (in terms of $m,n,$ and $k$), and then say what this means for our dataset, in particular with $k=2$, in comparison to the total number of entries in $P$.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############################################################################\n",
    "### This is a Markdown cell - Type Answer for Project Question 3 here ###\n",
    "############################################################################\n",
    "\n",
    "Lets assume there are k features of books and hence there will k affinities of users to those features.\n",
    "\n",
    "P = A . F\n",
    "\n",
    "A is matrix of shape (n x k) and F is a matrix of shape (k x m) and P is a matrix of shape (n x m)\n",
    "\n",
    "############################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <a name=\"4\">4 Baseline Model</a> \n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "In your computation above, you should have seen that the total number of parameters is vastly smaller than the total number of entries in the matrix $P$.  This is a key component of the model: because we need to only find the value for a comparatively small number of parameters to specify all of $P$, we will force our model to hunt for structure in the observations rather than just memorize scores.\n",
    "\n",
    "Let's consider a simple example of this type of model.  Set $k=1$.  Pick the single feature as the average star rating a book received, and everyone has affinity of $+1$ for that feature.  When you do the product of $P=AF$, note that this means that it predicts every single user gives the same rating to the ASIN, and that rating is the average value.  Throughout this document, we will refer to this as the __baseline model__.  It will give us a simple benchmark against which to measure our progress.\n",
    "\n",
    "We will dive deeper into why with next questions, but it is reasonable to evaluate these predictions by computing the average squared difference between the predictions and the true values in the *test* set, where the test set can be any set we would like to test our model against (the train or the validation set, for example):\n",
    "\n",
    "$$\n",
    "{\\frac{1}{\\#\\text{Test data points}}\\sum_{\\text{Test data}}(\\text{prediction} - \\text{true test observation})^2}\n",
    "$$\n",
    "\n",
    "This is called the mean squared error (or MSE for short). MSE has the same issue the variance has that we mentioned in class for the variance (that the units are uninterpretable), so traditionally we use the Root Mean Square (RMS) error which can be thought of how far our predictions are off on average and is obtained by taking the square root of the MSE:\n",
    "\n",
    "$$\n",
    "\\sqrt{{\\frac{1}{\\#\\text{Test data points}}\\sum_{\\text{Test data}}(\\text{prediction} - \\text{true test observation})^2}}.\n",
    "$$\n",
    "\n",
    "\n",
    "### Project Question 4\n",
    "> Write code to compute the vector of average ASIN scores from the training set as our single feature.  Construct a vector of all ones ([np.ones](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ones.html) will do this for you) for your vector of affinities, and then use these two vectors to create a (baseline) matrix of predictions $P$. Print the matrix $P$ and examine its elements. How does it compare to the validation matrix $S$? Compute the MSE and the RMS on the validation set. How far off are we on average if we use this simple method for prediction?\n",
    "\n",
    "For ease of implementation, note that RMS is the same as\n",
    "\n",
    "$$\n",
    "RMS = \\frac{1}{\\sqrt{\\text{Test data points}}}\\|R\\circ(S-P)\\|_2\n",
    "$$\n",
    "\n",
    "where we have use $R$ to mask off only the observed test entries, and the fact that the $L_2$ norm is nothing but the square root of the sum of the squares of entries. Using matrix products (implemented with ```np.dot(A, B)```, the Hadamard product (implemented with ```np.multiply(A, B)``` or ```A*B```), and ```np.linalg.norm```, computing this is a single line long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1490, 1186)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7322761194029851"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Implement Code for Project Question 4 here \n",
    "f = np.array(np.round(train.groupby(['ASIN']).mean()['Rating'],0))\n",
    "a = np.ones((s_train.shape[0],1))\n",
    "p = np.round(np.multiply(a,f),0)\n",
    "print(p.shape)\n",
    "error = (np.multiply(r_val, (s_val - p)))\n",
    "mse_baseline = np.square(np.linalg.norm(error)) * (1/val.shape[0])\n",
    "mse_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1490, 1186)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5337784760408484"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Implement Code for Project Question 4 here \n",
    "f = np.ones((s_train.shape[1],1))\n",
    "a = np.array(np.round(train.groupby(['User']).mean()['Rating'],0))\n",
    "p = np.round(np.multiply(a.T,f),0).T\n",
    "print(p.shape)\n",
    "error = (np.multiply(r_val, (s_val - p)))\n",
    "mse_baseline = np.square(np.linalg.norm(error)) * (1/val.shape[0])\n",
    "mse_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <a name=\"5\">5 Model Likelihood</a> \n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "\n",
    "#### Propose a model to capture the underlying relationship in the dataset  \n",
    "\n",
    "Our next investigations will be entirely theoretical and will contain no coding. We need to see how to bring randomness into the model that we defined.  The model we proposed so far was completely deterministic, we basically gave that our predictions are given by the product $P=AF$.  There was no allowance for error, and we need to fix that.\n",
    "\n",
    "We originally specified that we wanted our scores to be obtained as $AF$.  Unless the universe follows our model exactly (that there are truly just $k$ numbers that determine how much a person likes a book and the person assigns that score every single time without fail) we will not have that $S = AF$.  Instead, we will have that $S$ looks like $P=AF$ plus some __independent random noise__, where that random noise is designed to capture all the inaccuracies in our model (that there are many features that determine how much a person will like a book, and that there can be random fluctuation in the rating given depending on things unrelated to the book at all).\n",
    "\n",
    "We will do this by saying that $S \\sim P + \\mathcal{N}(0,\\sigma^2)$ where we choose to model our noise as independent additive Gaussian noise.  The way to read this is that every rating we observed $s_{i,j}$ is distributed as a our prediction $p_{i,j}$ plus independent mean zero Gaussian noise of unknown variance for every observation.  Since adding a constant to a Gaussian just shifts the mean, we could also say that $s_{i,j}\\sim\\mathcal{N}(p_{i,j},\\sigma^2)$.  The scale of the noise, $\\sigma^2$, is now an additional parameter of our model.\n",
    "\n",
    "\n",
    "#### Compute the likelihood that this model generates the data\n",
    "\n",
    "For many __independent observations__, as we assume our ratings are, each with its own Gaussian probability densities as proposed above, the probability to observe all datapoints (all ratings) at the same time is computed by multiplying the individual probabilities together. We can thus compute the probability densities of our observations.  As a matter of terminology, the probability or probability density of obtaining a particular set of data is often called the __likelihood__.\n",
    "\n",
    "\n",
    "### Project Question 5 \n",
    "> Let $N$ be the total number of observed ratings (of the training dataset).  For our model, explain why the likelihood is:\n",
    "\n",
    "$$ \n",
    "\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)^N e^{-\\frac{1}{2\\sigma^2}\\sum_{N}(s_{i,j} - p_{i,j})^2}. \n",
    "$$\n",
    "\n",
    "> Please write the derivation below.\n",
    "\n",
    "It helps to know that ```\\prod_{}``` in markdown is used to write down product of many things, just like ```\\sum_{}``` in markdown is used to write the sum of many things. Also useful to remember one property of the exponential function: $e^a e^b = e^{a+b}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############################################################################\n",
    "### This is a Markdown cell - Type Answer for Project Question 5 here ###\n",
    "############################################################################\n",
    "\n",
    "We established above that all the observations follow the gausian distribution with mean = pij and sd = sigma. Hence the likelihood or probability of observing a datapoint in a test is always equal to the above formula because we are taking product of independent probabilities.\n",
    "\n",
    "############################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <a name=\"6\">6 Loss Function</a> \n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "\n",
    "#### Compute the negative log-likelihood (loss function)\n",
    "\n",
    "The key to the whole maximum likelihood estimation process is that we want to find out what values of the model parameters make the data most likely (occur with the highest probability).  Notice that many of the terms above in the likelihood we computed are unrelated to the dataset, such as the term in front of the exponential.  Thus if we want to find the choice of predictions $p_{i,j}$ that maximizes the probability, or the likelihood rather, and we don't care about the value of $\\sigma^2$, we can throw away almost everything above.\n",
    "\n",
    "\n",
    "### Project Question 6\n",
    "> Explain that if you discard all terms that do not involve our data, the $-\\log$-likelihood (the __loss function__) is essentially:\n",
    "\n",
    "$$\n",
    "\\sum_{N}(s_{i,j} - p_{i,j})^2.\n",
    "$$\n",
    "\n",
    "> Then explain why minimizing this is the same thing as minimizing the MSE error, and thus we see that the intuitively reasonable MSE error is the same thing as maximizing the probability of our data.\n",
    "\n",
    "It helps to remember some of the logarithms properties: $\\log(ab) = \\log(a) + \\log(b)$, $\\log(a^N) = N \\log(a)$, and $\\log(e) = 1$. However, the question can be answered even without using negative log, ust need to notice that $e^{-x}$ is a decreasing function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############################################################################\n",
    "### This is a Markdown cell - Type Answer for Project Question 6 here ###\n",
    "############################################################################\n",
    "\n",
    "taking log of the above probability density function will result in the mentioned formula. \n",
    "Also log(a.b) = loga + logb\n",
    "\n",
    "############################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <a name=\"7\">7 Gradient Descent</a> \n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "\n",
    "#### Find the model that minimizes the loss function\n",
    "\n",
    "We will use a __gradient descent__ implementation to get this done, and consequently __learn the model from the train data__.\n",
    "\n",
    "Note that while we often talked about minimizing the RMS (root mean squared error) because it was more interpretable, we can just as freely optimize the MSE since the square root would just make the math harder.  This freedom is useful: we can think about RMS if we want to interpret what we see, and we can think about the MSE if we want to computationally optimize the model.\n",
    "\n",
    "Now, before diving into a computation let's take a moment and make sure our goals are clear.  We saw that it was reasonable to make our predictions using the product $AF$, since it corresponded to the idea that users have various preferences for various features of each ASIN.  If this was not a ML problem, we'd go through and assign to every ASIN a list of features by hand (perhaps using Mechanical Turk), and then hand code rules like, \"if a person has assigned 5 stars to at least 10 sci-fi books, then they have affinity $+1$ for sci-fi books.\"  This would likely work, but there is a lot of trial and error, and human labor.\n",
    "\n",
    "The ML solution is to essentially work this problem backwards.  We suppose that such features exist, we define a way to evaluate how well a collection of features and affinities works to describe the scores we observe, and then we try to automatically find the best possible set of features and affinities to explain the data we've seen.  Thus, our goal is to find the best possible values for $A$ and $F$ in the sense that they minimize the MSE.\n",
    "\n",
    "We can now finally fully dive in to the implementation of gradient descent for our recommender dataset. Let's summarize what we have so far on our mean squared error loss function that can be written as\n",
    "\n",
    "$$\n",
    "L(A,F) = \\frac{1}{N}\\|R\\circ(S-AF)\\|_2^2,\n",
    "$$\n",
    "\n",
    "where the variables have the following meanings:\n",
    "* $N$ is the number of data points in the dataset used to define $R$ or $S$.  Depending on the reason we are computing this, we could be dealing with either the training or testing data.\n",
    "* $k$, while not contained in the above formula, is the number of features describing each ASIN.\n",
    "* $R$ is the matrix that masks off those entries for which we have a score.\n",
    "* $S$ is the matrix of observed scores.\n",
    "* $A$ are the affinities of the users for various features.\n",
    "* $F$ are the values of those features for all the ASINs.\n",
    "* $AF$ is the matrix product that gives us our predictions for every pair of user and ASIN.\n",
    "\n",
    "To perform our gradient descent, we must first compute the gradient of our mean square errors loss function with respect to the parameters of our model: the entries of $A$ and $F$. While this loss function is still simple enough so we can work with this mathematically to find its derivatives, here we discuss how to approach this with autograd for automate computing the gradients!\n",
    "\n",
    "\n",
    "#### Implementing Gradient Descent with autograd\n",
    "\n",
    "In order to implement gradient descent to solve our problem we need to define a few things:\n",
    "* How do we initialize $A$ and $F$?\n",
    "* What will we take for the number of features/affinities ($k$)?\n",
    "* What will be our learning rate?\n",
    "* When will we stop our optimization?\n",
    "\n",
    "To find the best, we would want to do some *hyperparameter tuning*, which is to say optimization of the choices we make before running our optimization algorithms.  The accelerator classes discuss this at lenght, so we'll just take some values that work fairly well:\n",
    "* Initialize $A$ and $F$ with random real values in the interval $[0,1]$ (```np.random.rand``` will be useful).  \n",
    "* Take the number of features $k=2$. \n",
    "* Take the learning rate to be $25.0$\n",
    "* Stop after $300$ steps.\n",
    "\n",
    "\n",
    "### Project Question 7\n",
    "> Implement the gradient descent algorithm. To monitor learning of your model, plot the MSE error on both the train and validation data as a function of the number of iterations.  How does the final validation loss compare with the value you got using the baseline model from Question 4? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the P matrix (1490, 1186)\n",
      "0.39941493932273053 0.42691527387694245\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "np.random.seed(42)\n",
    "\n",
    "def loss_af(R, S, a,f, N, l1,l2):\n",
    "    p = np.dot(a,f)\n",
    "    delta = S - p\n",
    "    se = np.linalg.norm(np.multiply(R, delta))\n",
    "    return (np.square(se)  + (l1*np.square(np.linalg.norm(a))) + (l2*np.square(np.linalg.norm(f))))* (1/N)\n",
    "\n",
    "def grad_a(r_train, s_train, a,f,N,l1):\n",
    "    return (np.dot(np.multiply(r_train, (s_train - np.dot(a,f))), f.T)  - (2*l1*a)) * (2/N)\n",
    "\n",
    "def grad_f(r_train, s_train, a,f,N,l2):\n",
    "    return (np.dot(a.T, np.multiply(r_train, (s_train - np.dot(a,f)))) - (2*l2*f)) * (2/N)\n",
    "\n",
    "k = 1\n",
    "l1 = 0.007\n",
    "l2 = 0.002\n",
    "eta_a = 5\n",
    "eta_f = 100\n",
    "epochs = 300\n",
    "\n",
    "params = {'k': k,\n",
    "          'l1': l1,\n",
    "          'l2': l2,\n",
    "          'eta_a': eta_a,\n",
    "          'eta_f': eta_f,\n",
    "          'epochs': epochs\n",
    "         }\n",
    "\n",
    "f = np.random.normal(0,1,size=(k,s_train.shape[1]))\n",
    "a = np.random.normal(0,1,size=(s_train.shape[0],k))\n",
    "print(\"Shape of the P matrix\", np.dot(a,f).shape)\n",
    "\n",
    "N_train = train.shape[0]\n",
    "N_val = val.shape[0]\n",
    "\n",
    "training_loss = [loss_af(r_train, s_train, a,f, N_train,l1,l2)]\n",
    "validation_loss = [loss_af(r_val, s_val, a, f, N_val,l1,l2)]\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    f = f + eta_f*grad_f(r_train, s_train, a,f,N_train, l2) \n",
    "    a = a + eta_a*grad_a(r_train, s_train, a,f, N_train, l1)\n",
    "    \n",
    "    train_loss = loss_af(r_train, s_train, a,f, N_train,l1,l2)\n",
    "    val_loss = loss_af(r_val, s_val, a, f, N_val,l1,l2)\n",
    "    \n",
    "    training_loss.append(train_loss)\n",
    "    validation_loss.append(val_loss)\n",
    "\n",
    "print(train_loss, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.99814885, 4.11294325, 4.14791559, ..., 3.99808028, 3.97315388,\n",
       "        4.21030513],\n",
       "       [4.4077984 , 4.53435462, 4.57291021, ..., 4.40772281, 4.38024245,\n",
       "        4.64169218],\n",
       "       [4.9185649 , 5.05978619, 5.10280952, ..., 4.91848055, 4.88781582,\n",
       "        5.1795618 ],\n",
       "       ...,\n",
       "       [3.65690881, 3.76190558, 3.79389304, ..., 3.6568461 , 3.63404715,\n",
       "        3.85095767],\n",
       "       [4.86640143, 5.00612501, 5.04869206, ..., 4.86631798, 4.83597847,\n",
       "        5.12463035],\n",
       "       [4.14711596, 4.26618749, 4.30246286, ..., 4.14704484, 4.1211897 ,\n",
       "        4.36717698]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(a,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_training = training.pivot(index='User', columns='ASIN', values='Rating').to_numpy()\n",
    "s_training[np.isnan(s_training)] = 0\n",
    "r_training = s_training.copy()\n",
    "r_training[r_training > 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the P matrix (1490, 1186)\n",
      "0.4010522475873088\n"
     ]
    }
   ],
   "source": [
    "k = 1\n",
    "l1 = 0.007\n",
    "l2 = 0.002\n",
    "eta_a = 5\n",
    "eta_f = 100\n",
    "epochs = 300\n",
    "\n",
    "f = np.random.normal(0,1,size=(k,s_train.shape[1]))\n",
    "a = np.random.normal(0,1,size=(s_train.shape[0],k))\n",
    "print(\"Shape of the P matrix\", np.dot(a,f).shape)\n",
    "N_train = training.shape[0]\n",
    "\n",
    "training_loss = []\n",
    "training_loss.append(loss_af(r_training, s_training, a,f, N_train,l1,l2))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    f = f + eta_f*grad_f(r_training, s_training, a,f,N_train, l2) \n",
    "    a = a + eta_a*grad_a(r_training, s_training, a,f, N_train, l1)\n",
    "    \n",
    "    train_loss = loss_af(r_training, s_training, a,f, N_train,l1,l2)\n",
    "    \n",
    "    training_loss.append(train_loss)\n",
    "        \n",
    "print(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.70115376, 1.75043839, 1.78421909, ..., 5.87671434, 5.89620442,\n",
       "       5.93596512])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = np.dot(a,f)\n",
    "np.unique(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "P[P > 5] = 5\n",
    "P[P < 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ### Implement Code for Project Question 7 here \n",
    "# import numpy as numpy\n",
    "# from mxnet import autograd, np, npx\n",
    "# from mxnet.random import seed\n",
    "# import random\n",
    "\n",
    "# npx.set_np()\n",
    "\n",
    "# f = np.random.normal(0,1,size=(3,s_train.shape[1]))\n",
    "# a = np.random.normal(0,1,size=(s_train.shape[0],3))\n",
    "\n",
    "# print(\"Shape of the P matrix\", np.dot(a,f).shape)\n",
    "\n",
    "# r_train_mxnet = np.array(r_train)\n",
    "# s_train_mxnet = np.array(s_train)\n",
    "\n",
    "# def loss_af(R, S, params, N):\n",
    "#     a, f = params[0], params[1]\n",
    "#     P = np.dot(a,f)\n",
    "#     delta = S - P\n",
    "#     mse = np.linalg.norm(np.multiply(R, delta))\n",
    "#     return (mse**2)/ N\n",
    "\n",
    "# params = [a, f]\n",
    "# for param in params:\n",
    "#     param.attach_grad()\n",
    "\n",
    "# print(loss_af(r_train_mxnet, s_train_mxnet, params, train.shape[0]))\n",
    "\n",
    "# out_a = [params[0]]\n",
    "# out_f = [params[1]]\n",
    "\n",
    "# eta = 0.1\n",
    "# epochs = 300\n",
    "# training_loss = []\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     with autograd.record():\n",
    "#         loss = loss_af(r_train_mxnet, s_train_mxnet, params, train.shape[0])\n",
    "#     loss.backward()\n",
    "#     training_loss.append(loss)\n",
    "#     for param in params:\n",
    "#         param[:] = param - eta * param.grad\n",
    "#     out_a.append(params[0])\n",
    "#     out_f.append(params[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <a name=\"8\">8 Overfitting</a> \n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "If all goes well over the training period, the training and validation losses are both decreasing over the entire run (or do they?!), and probably finally end up somewhere lower than the baseline values, so there is an improvement in recommendation quality over the baseline model! However, let's examine at little bit closer the training and validation losses, to make sure they are indeed both decreasing over the entire run as expected. \n",
    "\n",
    "\n",
    "### Project Question 8\n",
    "> Plot the training and validation losses from the previous question, but now only do so for steps $50$ through $300$.  There were many large gains in the early steps of gradient descent that make the overfitting harder to see which should now become clearer.  A key technique is called *early stopping*, where you simply stop training your model at the point at which the validation losses were best.  What was the minimum value of the validation loss you obtained?  \n",
    "\n",
    "This question is a preview of one of the most pervasive issues you encounter in machine learning: __the phenomena of overfitting__. This is the issue of a model memorizing random fluctuations rather than true patterns. There is a hallmark of overfitting, which is that the models ability to predict test data becomes *worse* after continued training, even though the models ability to explain training data becomes better.  The idea is that in the beginning, it is learning real and generalizable patterns, however as it keeps learning for longer it starts learning things which it *thinks* are real patterns, but are instead just random fluctuations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f6f84765e48>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_loss[200:])\n",
    "plt.plot(validation_loss[200:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <a name=\"9\">9 Recommendations</a> \n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Let's finally have a look at our recommender system. That is, the last $P$ matrix from the gradient descent algorithm! Print it and have a look at its elements. All those empty entries of the $S$ matrix are now filled in with what are in fact recommendations for those users that have not yet rated those ASINs.\n",
    "\n",
    "This means that now that we have a trained recommender model, we can find all user-ASIN pairs in the training set where the user has not rated an ASIN, and make a recommendation based on the entries of the prediction matrix for that particular user-ASIN pair. \n",
    "\n",
    "\n",
    "### Project Question 9\n",
    "\n",
    "> Machine learning problems are often solved by carefully tuning and/or adding in more and more ingredients which slowly improve the model performance little by little. Improve model performance, then use the following code to extract the test predictions from the prediction matrix. Print out a sample of the test predictions, and have a look at the values. If all goes well, these should be values roughly between 1 and 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.838333423132668, 4.193378310457612, 5.0, 4.787716579315998, 4.963415392439763, 4.58757562189902, 4.774057313939899, 5.0, 4.629601428003545, 4.3124033271222295]\n"
     ]
    }
   ],
   "source": [
    "### Implement Code for Project Question 9 here ###\n",
    "###################################################\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# This is just a sample predictions of all ones\n",
    "# REPLACE WITH YOUR P matrix \n",
    "\n",
    "# Get predictions for the test_features set from the prediction matrix P \n",
    "test_predictions = []\n",
    "for index, row in test_features.iterrows():\n",
    "    userIdx = uniqueUsers.index(row['User'])\n",
    "    ASINIdx = uniqueASINs.index(row['ASIN'])\n",
    "    test_predictions.append(P[userIdx, ASINIdx])\n",
    "\n",
    "# This is just a sample test predictions of all ones\n",
    "# REPLACE WITH YOUR TEST PREDICTIONS\n",
    "print(test_predictions[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.85733172, 1.95921494, 1.9660451 , ..., 4.9999743 , 4.99999418,\n",
       "       5.        ])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <a name=\"10\">10 Submit to Leaderboard</a> \n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Although we can repeat Question 9 on the train and validation dataset, extracting the train and validation predictions from the prediction matrix and compare them against the real train and validaiton ratings (that we can extract from the ```train``` the ```val``` datasets), we would not be able to know how well the recommender system works on the test dataset unless we submit to the Leaderboard! \n",
    "\n",
    "\n",
    "### Project Question 10\n",
    "\n",
    "> Use the following code to write the test predictions to a CSV file (in the format expected by the Leaderboard). Download locally the CSV file from the SageMaker instance folder, and upload it to [https://leaderboard.corp.amazon.com/tasks/665/submit](__https://leaderboard.corp.amazon.com/tasks/665/submit__). Good luck!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ID    Rating\n",
      "0  84634  4.838333\n",
      "1  55852  4.193378\n",
      "2  28875  5.000000\n",
      "3  94134  4.787717\n",
      "4  31501  4.963415\n"
     ]
    }
   ],
   "source": [
    "### Implement Code for Project Question 10 here ###\n",
    "###################################################\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Get test predictions in the format expected by the Leaderboard\n",
    "submission = pd.DataFrame(columns=[\"ID\", \"Rating\"])\n",
    "submission[\"ID\"] = test_features[\"ID\"]\n",
    "submission[\"Rating\"] = test_predictions \n",
    "\n",
    "submission.to_csv(\"regularized_gradient_descent_v_final.csv\", index=False)\n",
    "\n",
    "# This is just a sample test predictions of all ones\n",
    "# REPLACE WITH YOUR TEST PREDICTIONS\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.371, 5.0]                   17916\n",
       "(3.743, 4.371]                  7383\n",
       "(3.114, 3.743]                  1598\n",
       "(2.486, 3.114]                   236\n",
       "(1.8530000000000002, 2.486]       25\n",
       "Name: Rating, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission['Rating'].value_counts(bins=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "submission['Rating'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <a name=\"100\">Final Comments</a> \n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Once you get your final metric and compare it to the original rating, you might find yourself disappointed that it wasn't too much better.  I will say that the $14\\%$ improvement you should have seen is actually quite large.  This is the way of things in most ML projects. Something very near this algorithm is what gave Netflix its early lead in the movie recommendation market in the early 2000s.  They were only able to decrease the error by another $0.1$ stars after offering a [million dollar prize](https://en.wikipedia.org/wiki/Netflix_Prize) which took $3$ years of joint academic and industry effort to claim!  \n",
    "\n",
    "You will often times be able to implement a trivial solution in minutes to do an ok job, work for weeks to improve on it by $10\\%$, and then years to get another $10\\%$.  Reading the [writeup](http://www.netflixprize.com/assets/ProgressPrize2008_BellKor.pdf) by the winning team will let you get a feeling of how subtle these gains often are!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
